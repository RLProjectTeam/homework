1. memory的模块: (0)把代码改成在gpu上运行  【已完成 Zhu】
                （1）简单修改：引入权重 alice_history = alice_reward * history 【已完成】；
                              只把Bob没有完成的那次history放进alice memory【已完成】
                              memory中融入每一个回合Bob完成的情况的信息(例如Bob的Reward)
                 
                 (2)新思路修改：非对称self play ---》 对称self play（加快预训练过程，更容易收敛）
                 
                 插一句前面的话题（对称self play)，不知道理解对没有：就是用一个网络，决定Alice怎么走，Bob怎么走；第一轮：alice作为老师，bob来重复，然后用Alice的reward来更新这个网络；第2轮，Bob作为老师，Alice来重复，然后用Bob的reward来更新这个网络？
                 
                 
                 
                 （3）self play阶段的reward修改：
                 试想在self play的环节Alice和Bob各执自己的目的 在走每一步都要进行决策 可是并不是每走一步就有一个指导 而是在走了好几十步才有一个Reward对他们的行为进行指导，sparse reward


                (1) 把最近的k个作为lstm的输入【忽略，修改无意义】
                (2) 将memory改成多层级的 将trajecoty输入到lstm中 得到当前回合的history
                        (a) 使用单向的lstm
                        (b) 使用双向的lstm
                (3) 将lstm改为其他网络结构(GRU)
                (5) memory只记录Bob最差的k个history 或者 memory只记录Bob能完成的最差的k个hisory
      
